{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_records_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CUE', 'TARGET', 'NORMED?', '#G', '#P', 'FSG', 'BSG', 'MSG', 'OSG',\n",
       "       '#M', 'MMIAS', '#O', 'OMIAS', 'QSS', 'QFR', 'QCON', 'QH', 'QPS', 'QMC',\n",
       "       'QPR', 'QRSG', 'QUC', 'TSS', 'TFR', 'TCON', 'TH', 'TPS', 'TMC', 'TPR',\n",
       "       'TRSG', 'TUC', 'TRF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NORMED?</th>\n",
       "      <th>#G</th>\n",
       "      <th>#P</th>\n",
       "      <th>FSG</th>\n",
       "      <th>BSG</th>\n",
       "      <th>MSG</th>\n",
       "      <th>OSG</th>\n",
       "      <th>#M</th>\n",
       "      <th>...</th>\n",
       "      <th>TSS</th>\n",
       "      <th>TFR</th>\n",
       "      <th>TCON</th>\n",
       "      <th>TH</th>\n",
       "      <th>TPS</th>\n",
       "      <th>TMC</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TRSG</th>\n",
       "      <th>TUC</th>\n",
       "      <th>TRF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>NO</td>\n",
       "      <td>152</td>\n",
       "      <td>69</td>\n",
       "      <td>0.454</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>...</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>ALPHABET</td>\n",
       "      <td>YES</td>\n",
       "      <td>152</td>\n",
       "      <td>10</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>THE</td>\n",
       "      <td>NO</td>\n",
       "      <td>152</td>\n",
       "      <td>10</td>\n",
       "      <td>0.066</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>...</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>¥</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>GRADE</td>\n",
       "      <td>YES</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>3.7</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>LETTER</td>\n",
       "      <td>YES</td>\n",
       "      <td>152</td>\n",
       "      <td>6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>145</td>\n",
       "      <td>5.16</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CUE    TARGET NORMED?   #G  #P    FSG    BSG    MSG     OSG #M  ... TSS  \\\n",
       "0   A         B      NO  152  69  0.454      ¥      ¥       ¥  ¥  ...   ¥   \n",
       "1   A  ALPHABET     YES  152  10  0.066  0.046  0.002       0  2  ...  11   \n",
       "2   A       THE      NO  152  10  0.066      ¥      ¥       ¥  ¥  ...   ¥   \n",
       "3   A     GRADE     YES  152   9  0.059  0.277      0  0.0013  0  ...  14   \n",
       "4   A    LETTER     YES  152   6  0.039      0  0.003  0.0022  2  ...  15   \n",
       "\n",
       "   TFR  TCON   TH  TPS   TMC   TPR   TRSG  TUC  TRF  \n",
       "0    ¥     ¥  NaN  NaN     ¥     ¥      ¥  0.0  NaN  \n",
       "1    2     ¥  NaN    N   0.5  0.25  0.062  1.0  NaN  \n",
       "2    ¥     ¥  NaN  NaN     ¥     ¥      ¥  0.0  NaN  \n",
       "3   35   3.7    N    N     1  0.54  0.025  1.0  NaN  \n",
       "4  145  5.16    N    N  1.27   0.6  0.142  1.0  NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the required columns\n",
    "df = df[['CUE','TARGET','NORMED?','#G','#P','FSG','BSG','QPS','TPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUE Normed Word\n",
    "- TARGET Response to Normed Word\n",
    "- NORMED? Is Response Normed?\n",
    "- #G Group size\n",
    "- #P Number of Participants Producing Response\n",
    "- FSG Forward Cue-to-Target Strength\n",
    "- BSG Backward Target-to-Cue Strength\n",
    "- QPS Cue: Part of Speech\n",
    "- TPS Target: Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters used to clean the data\n",
    "df = df[df['NORMED?']=='YES']\n",
    "df['BSG'] = df['BSG'].astype(float)\n",
    "df = df[df['BSG']==0]\n",
    "df = df[df['#P']>2].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering POS for CUES and TARGETS to be, nouns verbs and adjectives\n",
    "exclude_list = ['PP','AV','ADV','P','I','C','AD']\n",
    "df1 = df[~df['QPS'].str.contains('|'.join(exclude_list))]\n",
    "df1['TPS'] = df1['TPS'].dropna()\n",
    "df1 = df1[df1['TPS']!=' ']\n",
    "df1 = df1[df1['TPS']!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N: Noun\n",
    "- V: Verb\n",
    "- AJ: Adjective\n",
    "- AD: Adverb\n",
    "- P: Pronoun\n",
    "- PP: Preposition\n",
    "- I: Interjection\n",
    "- C: Conjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N      16739\n",
       "AJ      4321\n",
       "V       4047\n",
       "AD       425\n",
       "P         75\n",
       "ADJ       71\n",
       "PP        55\n",
       "I         33\n",
       "A          6\n",
       "INT        4\n",
       "AV         3\n",
       "PRP        2\n",
       "C          2\n",
       "Name: TPS, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting count for each Part-of-Speech\n",
    "df1['TPS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_list = ['N','V','AJ','AD','P','PP','I','C']\n",
    "df1 = df1[df1['TPS'].str.contains('|'.join(include_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the edge weights using FSG\n",
    "\n",
    "df_group = df1.groupby(['QPS','TPS']).mean()\n",
    "df_group = df_group.reset_index()\n",
    "\n",
    "d_weights = {}\n",
    "for row in df_group.itertuples():\n",
    "    d_weights[(row.QPS,row.TPS)]=row.FSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Part-of-Speech network\n",
    "\n",
    "lst = ['N','AJ','V','AD','P','PP','I','C']\n",
    "G = nx.Graph()\n",
    "for n in lst:\n",
    "    G.add_node(n)\n",
    "for row in df1.itertuples():\n",
    "    if row.TPS in ['N','AJ','V','AD','P','PP','I','C']: \n",
    "        G.add_edge(row.QPS,row.TPS,weight = d_weights[(row.QPS,row.TPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Part-of-Speech network\n",
    "nx.write_gml(G,'POS.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Vowels Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the required data for vowels\n",
    "\n",
    "exclude_list = ['PP','AV','ADV','P','I','C']\n",
    "df2 = df[~df['QPS'].str.contains('|'.join(exclude_list))]\n",
    "df2['TPS'] = df2['TPS'].dropna()\n",
    "df2 = df2[df2['TPS']!=' ']\n",
    "\n",
    "\n",
    "df_vowels = df2[(df2['CUE'].str.startswith('A')) |\n",
    "               (df2['CUE'].str.startswith('E')) | \n",
    "               (df2['CUE'].str.startswith('I')) |\n",
    "               (df2['CUE'].str.startswith('O')) | \n",
    "               (df2['CUE'].str.startswith('U'))]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Vowels Association network\n",
    "\n",
    "ls_vowels = list(set(list(df_vowels['CUE'])+list(df_vowels['TARGET'])))\n",
    "G = nx.Graph()\n",
    "for n in ls_vowels:\n",
    "    G.add_node(n)\n",
    "for row in df_vowels.itertuples():\n",
    "    G.add_edge(row.CUE,row.TARGET)\n",
    "    \n",
    "vowels = df_vowels.groupby(['CUE', 'TARGET'])['FSG'].mean()\n",
    "vowel = {}\n",
    "for i,j in zip(vowels.index,vowels.values):\n",
    "    vowel[i] = {'weight':j}\n",
    "    \n",
    "nx.set_edge_attributes(G,vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Vowels Association network\n",
    "nx.write_gml(G,'cue_target_vowels.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Sub-Topic and Topic Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_hyp1 = []\n",
    "cue_hyp2 = []\n",
    "tar_hyp1 = []\n",
    "tar_hyp2 = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Getting the sub-topics and topics for the CUE words\n",
    "    try:\n",
    "        My_sysn = wordnet.synsets(df.loc[i,'CUE'])[0]\n",
    "        x = My_sysn.hypernyms()[0].name()\n",
    "        cue_hyp1.append(x.split('.')[0])\n",
    "        my_sysn2 = wordnet.synsets(x.split('.')[0])[0]\n",
    "        try:\n",
    "            y = my_sysn2.hypernyms()[0].name()\n",
    "            cue_hyp2.append(y.split('.')[0])\n",
    "        except:\n",
    "            cue_hyp2.append('NA')\n",
    "    except:\n",
    "        cue_hyp1.append('NA')\n",
    "        cue_hyp2.append('NA')\n",
    "    \n",
    "    # Getting the sub-topics and topics for the TARGET words\n",
    "    try:\n",
    "        My_sysn = wordnet.synsets(df.loc[i,'TARGET'])[0]\n",
    "        x = My_sysn.hypernyms()[0].name()\n",
    "        tar_hyp1.append(x.split('.')[0])\n",
    "        my_sysn2 = wordnet.synsets(x.split('.')[0])[0]\n",
    "        try:\n",
    "            y = my_sysn2.hypernyms()[0].name()\n",
    "            tar_hyp2.append(y.split('.')[0])\n",
    "        except:\n",
    "            tar_hyp2.append('NA')\n",
    "    except:\n",
    "        tar_hyp1.append('NA')\n",
    "        tar_hyp2.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the required data and filtering them\n",
    "\n",
    "df3 = df.copy()\n",
    "df3['cue_hyp1'] = cue_hyp1\n",
    "df3['cue_hyp2'] = cue_hyp2\n",
    "df3['tar_hyp1'] = tar_hyp1\n",
    "df3['tar_hyp2'] = tar_hyp2\n",
    "df3 = df3[['CUE','TARGET','cue_hyp1','cue_hyp2','tar_hyp1','tar_hyp2','FSG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>cue_hyp1</th>\n",
       "      <th>cue_hyp2</th>\n",
       "      <th>tar_hyp1</th>\n",
       "      <th>tar_hyp2</th>\n",
       "      <th>FSG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>LETTER</td>\n",
       "      <td>metric_linear_unit</td>\n",
       "      <td>linear_unit</td>\n",
       "      <td>document</td>\n",
       "      <td>writing</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>PLUS</td>\n",
       "      <td>metric_linear_unit</td>\n",
       "      <td>linear_unit</td>\n",
       "      <td>quality</td>\n",
       "      <td>attribute</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>ANIMAL</td>\n",
       "      <td>placental</td>\n",
       "      <td>mammal</td>\n",
       "      <td>organism</td>\n",
       "      <td>living_thing</td>\n",
       "      <td>0.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>ANT</td>\n",
       "      <td>placental</td>\n",
       "      <td>mammal</td>\n",
       "      <td>hymenopterous_insect</td>\n",
       "      <td>insect</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>MUSIC</td>\n",
       "      <td>placental</td>\n",
       "      <td>mammal</td>\n",
       "      <td>auditory_communication</td>\n",
       "      <td>communication</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26100</th>\n",
       "      <td>ZUCCHINI</td>\n",
       "      <td>VEGETABLE</td>\n",
       "      <td>marrow</td>\n",
       "      <td>connective_tissue</td>\n",
       "      <td>produce</td>\n",
       "      <td>food</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26101</th>\n",
       "      <td>ZUCCHINI</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>marrow</td>\n",
       "      <td>connective_tissue</td>\n",
       "      <td>chromatic_color</td>\n",
       "      <td>color</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26102</th>\n",
       "      <td>ZUCCHINI</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>marrow</td>\n",
       "      <td>connective_tissue</td>\n",
       "      <td>substance</td>\n",
       "      <td>matter</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26103</th>\n",
       "      <td>ZUCCHINI</td>\n",
       "      <td>BROCCOLI</td>\n",
       "      <td>marrow</td>\n",
       "      <td>connective_tissue</td>\n",
       "      <td>crucifer</td>\n",
       "      <td>herb</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26104</th>\n",
       "      <td>ZUCCHINI</td>\n",
       "      <td>NASTY</td>\n",
       "      <td>marrow</td>\n",
       "      <td>connective_tissue</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CUE     TARGET            cue_hyp1           cue_hyp2  \\\n",
       "0             A     LETTER  metric_linear_unit        linear_unit   \n",
       "1             A       PLUS  metric_linear_unit        linear_unit   \n",
       "2      AARDVARK     ANIMAL           placental             mammal   \n",
       "3      AARDVARK        ANT           placental             mammal   \n",
       "4      AARDVARK      MUSIC           placental             mammal   \n",
       "...         ...        ...                 ...                ...   \n",
       "26100  ZUCCHINI  VEGETABLE              marrow  connective_tissue   \n",
       "26101  ZUCCHINI      GREEN              marrow  connective_tissue   \n",
       "26102  ZUCCHINI       FOOD              marrow  connective_tissue   \n",
       "26103  ZUCCHINI   BROCCOLI              marrow  connective_tissue   \n",
       "26104  ZUCCHINI      NASTY              marrow  connective_tissue   \n",
       "\n",
       "                     tar_hyp1       tar_hyp2    FSG  \n",
       "0                    document        writing  0.039  \n",
       "1                     quality      attribute  0.033  \n",
       "2                    organism   living_thing  0.322  \n",
       "3        hymenopterous_insect         insect  0.197  \n",
       "4      auditory_communication  communication  0.020  \n",
       "...                       ...            ...    ...  \n",
       "26100                 produce           food  0.331  \n",
       "26101         chromatic_color          color  0.149  \n",
       "26102               substance         matter  0.088  \n",
       "26103                crucifer           herb  0.034  \n",
       "26104                      NA             NA  0.020  \n",
       "\n",
       "[26105 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the required data\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Sub-Topic network\n",
    "\n",
    "x = []\n",
    "x.extend(df3.tar_hyp1)\n",
    "x.extend(df3.cue_hyp1)\n",
    "x = set(x)\n",
    "\n",
    "# Adding the nodes\n",
    "G = nx.Graph()\n",
    "for i in x:\n",
    "    if i=='NA':\n",
    "        continue\n",
    "    G.add_node(i)\n",
    "    \n",
    "# Adding the edges\n",
    "for i,j in zip(df3['cue_hyp1'],df3['tar_hyp1']):\n",
    "    if i=='NA' or j=='NA':\n",
    "        continue\n",
    "    G.add_edge(i,j)\n",
    "    \n",
    "# Getting the edge weights\n",
    "df4 = df3[df3['cue_hyp2']!='NA']\n",
    "df4 = df4[df4['tar_hyp2']!='NA']\n",
    "hyp1 = df4.groupby(['cue_hyp1','tar_hyp1'])['FSG'].mean()\n",
    "\n",
    "first_hyp = {}\n",
    "for i,j in zip(hyp1.index,hyp1.values):\n",
    "    first_hyp[i] = {'weight':j}\n",
    "    \n",
    "nx.set_edge_attributes(G,first_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Sub-Topic network\n",
    "nx.write_gml(G,'first_hyp.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Topic network\n",
    "\n",
    "x = []\n",
    "x.extend(df3.tar_hyp2)\n",
    "x.extend(df3.cue_hyp2)\n",
    "x = set(x)\n",
    "\n",
    "# Adding the nodes\n",
    "G = nx.Graph()\n",
    "for i in x:\n",
    "    if i=='NA':\n",
    "        continue\n",
    "    G.add_node(i)\n",
    "    \n",
    "# Adding the edges\n",
    "for i,j in zip(df3['cue_hyp2'],df3['tar_hyp2']):\n",
    "    if i=='NA' or j=='NA':\n",
    "        continue\n",
    "    G.add_edge(i,j)\n",
    "    \n",
    "# Getting the edge weights\n",
    "df5 = df3[df3['cue_hyp2']!='NA']\n",
    "df5 = df5[df5['tar_hyp2']!='NA']\n",
    "hyp2 = df5.groupby(['cue_hyp1','tar_hyp1'])['FSG'].mean()\n",
    "\n",
    "second_hyp = {}\n",
    "for i,j in zip(hyp2.index,hyp2.values):\n",
    "    second_hyp[i] = {'weight':j}\n",
    "    \n",
    "nx.set_edge_attributes(G,second_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Topic network\n",
    "nx.write_gml(G,'second_hyp.gml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
